in pipeline design, mem access latency has became a bottleneck

the gap between CPU clock rate and mem access latency is increasing

caches in the hierarchy

why caches work? spatial(code,array)/temporal locality(reuse/loop)

cache structure:

direct mapped: addr -> tag + index(which cache line in the cache) + offset(byte select), every block in the memory has a fixed position to be put in the cache(so it's called a mapping)

problem: associativity conflict, 2 mem block are mapping into the same cache line, and they displace each other

N-way set associative cache: now every cache index has N slots available in the cache, cons: extra gate delays because of the mux, we should first check hit/miss and then fetch the cache line, but the data block is available before we can confirm hit/miss in the direct mapped cache

fully-associative cache: no indexes, every mem block can be placed everywhere in the cache, we may need to check every cache line to find out whether it misses

pros&cons: more comparators, larger, more energy; mitigate the associativity conflict problem since positions are not limited; thus better hit rate

design issues: more associativity, less index bits(no indexes in a fully-associative cache)

replacement: least-soon reused is ideal, least recently used is a good approximation, even random is good(comparable perf then LRU, LRU is better only on small caches)!

writing policies: write through always update cache and main mem at the same time; write back only write the data back to main mem once the data is displaced(clean or dirty)

pros&cons: WB don't need to write to a same location multiple times, thus CPU don't need to block or wait since the lower level of the mem hierarchy doesn't have enough bandwidth; WT always combines with a write buffer to mitigate this problem; WT can make the data on the main mem up to date, but cannot solve cache coherency problem in a SMP system

other topics: cache coherency/victim caches/prefetching

more threads can hide the latency of mem, exploit mem parallelism



