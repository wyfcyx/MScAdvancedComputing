ultimate goal: reduce average memory access time

today: reduce miss rate on HW



classifying misses: 3Cs

compulsory(cold start misses/first reference misses); capacity(cache cannot contain all the blocks which are needed); conflict(associativity is not enough)

4C->Coherence

diagram: as cache size or ways increases, miss rate decreases(main misses type from conflicts->capacity->compulsory when inf cache) for SPEC CPU92

in real apps, we always receive and work on new data, which means that the percentage of compulsory should be higher

SPEC CPU17: integer/float benchmarks, int focus (hardly rely on pointers) on prediction and parallelism, float more structured control; speed/rate metrics; base(all program use the same flags to compile)/peak(individual flags); how to average? geometric mean not arith mean when rates/ratios;



do not change total cache size: change block sz/associativity/compiler

> btw: set: number of cache blks which are indexed by a single cache index

## block sz

too small block sz: every miss cannot supply enough data for the cache

too big block sz: cache blocks are not fully utilized if cache block sz is small and fixed, higher miss rate!

*do not only consider miss rate*: larger cache block -> larger miss penalty

## asso

increase asso, makes cache more complicated(more deep path, more comparators and selectors, direct-mapped is the fastest since we can get the data block even before we complete comparing the tags), therefore cache hit time becomes longer, can makes AMAT worse when cache is large, but by using way predictor we can mitigate this problem

another way: hybrid method(we want both small hit time of direct-mapped cache and low miss rate of set-associative cache called 'victim cache'); a large direct-mapped cache and a small fully associative cache; first check large cache, if miss, check victim cache, if still miss; on replacement of large cache, allocate into victim cache to give it the second chance; if hit on victim cache, allocate back into large cache;

an example of competitive algorithms: combine two strategies to create a good composite strategy

more examples: ski rental problem; spin or context-switch; paging(like dating? choose this one or try to find the next one)



randomization; skewed-associative caches; each way has a different hash function f, cache block in this way is indexed by f(lower k bits of addr); advantage1: if f0(A)=f0(B)=f0(C) on way0, then it's unlikely that f1(A)=f1(B)=f1(C), which reduces conflicts, on conventional set-associative caches f0=f1 so they're all the same; advantage2: if A,B,C are arrays and f0(A)=f0(B)=f0(C), then it's unlikely that f1(A+1)=f1(B+1)=f1(C+1); more predictable **average** perf; but hard to reason about or write program accordingly; costs: one addr decoder per way, latency of hash(depends on the complexity of the hash function), difficulty of implementing LRU



alternative of exploiting spatial locality instead of large cache lines: HW prefetching; when large cache misses, also fetch the next cache line and store it in the stream buffer(nowadays they're stored directly into large cache cuz pollution is no longer a problems since larger cache sz/asso), but instructions do not need to wait for its completion; extension: multiple stream buffers; Q: prefetch N+k or N+1?; btw, SW prefetching: write code to trigger the prefetching mechanism 



more extreme idea: decoupled access-execute; A/E processor; A processor execute the instructions which generating addresses to access first; related to float-point unit?

## compiler

not covered yet