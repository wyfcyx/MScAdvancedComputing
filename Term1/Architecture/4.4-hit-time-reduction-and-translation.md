MIPS R4000, 8 stages pipeline: pipeline the cache during IF and MEM, more complicated, perf: load/branch stalls are small, fp stalls are big since it's in-order

pipeline the cache; increase throughput but cannot reduce latency; 1) divide the cache into multiple banks, use something like hash to locate an access; 2)multiple copies of cache, each allocate an entry when storing(like a multi-ports RAM which can access multiple words in a cycle)

virtual memory and address translation; multiple designs:

1) physically-indexed, physically tagged(PIPT); CPU -VA-> TLB -PA-> cache -PA-> RAM(longer hit time)
2) virtually-indexed, virtually tagged(VIVT); CPU -VA-> cache(indexed by VA tags) -VA-> TLB -PA-> RAM(shorted hit time, if cache hit do not need to do address translation, but encounter some problems)
3) virtually-indexed, physically tagged(VIPT); CPU -> in parallel { -offset-> cache(containing data and PA tags), -VPN-> TLB, compare} -PA-> level-2 cache -PA-> RAM

Paging & virtual memory; TLB: highly associative cache of Page Table; translation for process isolation; sparsely occupied huge virtual space->multi-level page table; share read-only data between different processes(shared library); copy-on-write(not during forks), first mapped to a read-only zeroed page, if write to->exception->fix it by...; memory-mapped files(not enough details);

homonyms(same sound different meanings): same VA from different processes point to different PAs; if cache indexed by VA, have to flush cache during context switch which is very expensive; another solution is that just changing ASID(or PID)

synonyms(different sound same meanings): different VA from different processes point to the same PA; problem: if cache indexed by VA, then consistency issues between two cache lines referring to the same physical location; solution: force indexing bits of two VAs to be the same(somewhat page coloring)

to be continued...