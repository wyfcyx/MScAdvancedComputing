write through vs. write back; through: up-to-date data & simple management; back: reduce required mem bandwidth,

write allocate vs. non-allocate; non-alloc when we know that some data won't be used again

write buffer between cache and mem; check write buffer first when loading; size: not need to be very big; coalesce many writes to the same cache line in the write buffer; check cache and write buffer in parallel just like victim cache?

copying data into cache line is a word-by-word sequential process; early restart: resume execute once the required word is loaded; critical word first: prioritize the loading of the required word; only useful for large cache lines, so *prefetching* is still a good alternative; per-sector, unit of data delivering, validity bits if different words are accessed

non-blocking or lockup-free cache: hit-under-miss, need more explanations...

add a second-lvl cache: local vs. global miss rate

example: miss penalty are different in different cases

multilevel inclusion: L_{n+1} caches contains everything in L_n; a common choice: allocate into L1/L2 but not LLC(last level cache) which doesn't satisfy multilevel inclusion; good thing: fast invalidation, if L2 does not contain then L1 neither;