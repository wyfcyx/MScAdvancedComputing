in FORTRAN, column-major storage layout is default

loop-tilling: idea is that we can control what to put in the cache at a time

MM4: 8192 is aligned to cache size, so `C[i][j]` and `B[k][j]` maps to the same place in direct mapping!; associativity won't help except for when it increases to the block sz(32)

*never write you own MM*!

Intel compiler can automatically vectorize loop & array for MM1/MM2, they even run faster than MM3/MM4

> rethink MM experiment:
>
> `sim-cache` is a direct-mapped cache, block size is 32B, but the number of sets can be adjusted from 64 to 8K, which mean that total size can vary from 2KiB to 256KiB.
>
> And we can also try to extract the CPU info, `cat /proc/cpuinfo` on texel machines, I found that there're many processors each of which has multiple cores, which made me confused. But we can focus on the cache size, it's 8192KiB! We may still want to know its associativity and its set number. More information: 4cores/processor, L1 data cache: 8-way set associative 32KiB per core, L2 cache: 4-way set associative 256KiB per core, L3 cache: 16-way set associative 2MiB per core or 8MiB shared, so 8MiB is the L3 cache.
>
> what MM3 is trying to do: from MM2's ikj method, consider the inner loop, $\text{kk}\leq k < \text{kk+BSZ},\text{jj}\leq\text{jj+BSZ}$, so $c[i][j]$ is a part of a row which contains $\text{BSZ}$ elements, $a[i][k]$ is a constant since $i,k$ are known, and $b[k][j]$ is also a part of a row. Here we set $\text{BSZ}$ to 32, so 32 doubles are 256B, if cache size > 512B, then we can expect that there'll be no conflicts(a larger cache is better!). This is how we can take advantage of the cache independent of the problem size. 
>
> why MM4 is slower than MM3: each double is 8B, so each row is 8192x8B=64KiB, go back to `sim-cache` default configuration, each cache line can store 4 doubles, ...
>
> how to visualize them! Now the whole cache can be seen as a cycle which is divided into many parts, with the size of each part being the cache block size. in MM2, 



ISA without registers(actually show data flow in Tomasulo's perspective): stack machine(even in a stack HW, what if it overflows)/dataflow machine/'belt' machine(every inst writes to a fresh register, a finite window contains some regs, inst can refer to other inst's result by relative offsets)

dataflow ISA: TRIPS???