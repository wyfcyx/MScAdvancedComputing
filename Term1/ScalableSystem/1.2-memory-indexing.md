memory perf(latency) grow slowly the its capacity, 10% per year and doubled every 18 months respectively

memory latency->bottleneck, NUMA, cache perf is critical

align ds size according to the size of one or multiple cache lines, in this way we can avoid random access. in other words, we're trying to compress the data in the cache line.

cannot control cache directly(but I know we can control on ARM?)

cache indexing: B Tree, B+ Tree where there're links between leaf nodes, efficient when ranged queries, B+ Tree continue...

now cache sensitive search tree: aligned to L2 cache line, faster, smaller space, no explicit pointers,

CSB+ Tree/CSB+ Tree with segments/Full CSB+ Tree

B+ best for insertions/CSS best for RO/what else?

table split and join methods: multi-pass radix join method: extra cost but still better than cache miss penalty cuz it can avoid cache thrashing

take advantage of the cache: cluster data in a cache line/avoid cache thrashing/eliminate irrelevant data such as pointers in B+ tree

another example: spatial data, find all data in a K-dimension area, on disk 96% reading, on memory 95% for computation

how to partition: used to->fixed number of entries on a disk page, non-uniform, have to check entries may be very far from

core idea: eliminate unnecessary computations, scale from coarse to fine

compressing data, this may loss some precision, actually this makes the area bigger, but this doesn't affect the correctness(things change if they become smaller!)



what is fan-out?